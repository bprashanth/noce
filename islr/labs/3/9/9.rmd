## a. Produce a scatterplot matrix which includes all the variables in the dataset 

```{r}
auto <- read.csv("/home/beeps/rtmp/external/eco/noce/islr/labs/3/data/auto.csv")   
auto = na.omit(auto)
plot(auto)
```

## b. Compute the matrix of correlations between the variables using the function `cor()`. Exclude the name variable, which is qualitative. 

First preprocess the data to remove rows like
* sapply: applies the function mapping all values of auto to a numeric value. 
  - Non numerics are converted to NA. 
* subset: drops the name column, which is all NA 
* na.omit: omits other rows that have NA because of spurious data entries
```{r}
auto[33,]
auto_clean <- na.omit(subset(data.frame(sapply(auto, function(x) as.numeric(x))), select=-name))
cor(auto_clean)
```

## c. Use the lm function to perform a multiple linear regression with the mpg as the response and all other variables except name as the predictors    

```{r}
lm.ols = lm(mpg~., data=auto_clean)
summary(lm.ols)
```

i. Is there a relationship between the predictors and the response 

Yes, high F statistic with a low p value 

ii. Which predictors appear to have a statistically significant relationship to the response

weight, year, origin

iii. What does the coefficient for the year variable suggest

It suggests that newer cards have higher mpg (positive correlation). More specifically, for every one year, cards became more fuel efficient by 0.75. 

Whereas the coefficient of weight suggests that heavier cars have a lower miles per gallon. 

## d. Use the plot() function to produce diagnostic plots of the linear regression fit. 

```{r}
par(mfrow=c(2,2))
plot(lm.ols)
```

i. Comment on any problems you see with the fit. 

Evidence of non-linearity. The curve patter of the residual plot shows the function is non linear. 

ii. Do the residual plots suggest any unusually large outliers? 
iii. Does the leverage plot identify any observations with unusually high leverage?

Some residuals (point 14) are far outliers. To figure out further information about outliers we can:

a. plot the cooks distance cutoff with `abline`
```{r}
cooksd <- cooks.distance(lm.ols)
plot(cooksd, pch="*")
# Plot cutoff line at 4 * mean(cookds)
abline(h = 4*mean(cooksd, na.rm=T), col="red")
# Plot labels only if above abline 
text(x=1:length(cooksd) + 1, y=cooksd, labels=ifelse(cooksd>4*mean(cooksd, na.rm=T), names(cooksd), ""), col="red")
```

b. plot the studentized residual, values > 3 on the absolute scale are classified outliers. 
```{r}
plot(predict(lm.ols), rstudent(lm.ols))
```

## e. Use the * and : symbols to fit linear regression models with interaction effects. 

The following should introduct 3 terms, 2 individual and one interaction 
```{r}
summary(lm(mpg~displacement*cylinders+displacement*weight, data=auto))
```

Do any interactions appear to be statistically significant? 

One way to pick the terms for interaction is via correlation matrix. It looks like displacement:weight is significant while the interaction between displacement and cylinders is not. 

## f. Try a few different transformations of the variables such as log(x), root(x), x^2

* One applies root, log against skewed transformations (roots are weaker)


```{r}
summary(lm(mpg~log(weight)+sqrt(horsepower)+acceleration+I(acceleration^2), data=auto_clean))
```

* Worse R squared and better F than the regular OLS
* The residual vs fitted values show indications of heteroskedasticity (non uniform varience of residuals around the mean of the residuals - cone shaped, initially low, increasing)
* Multiple variables show an l like form when plotted against y, indicating that it might be better to take a log on the response variable (mpg) instead 

```{r}
summary(lm(log(mpg)~., data=auto_clean))
```

* This shows better R squared and F than the regular ols
* There is also no longer any evidence of hetereskedascity around the mean of the residuals 